import numpy as np

# Visualization libraries imported by chatGPT:
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
import seaborn as sns
from matplotlib.animation import FuncAnimation
import pandas as pd


class ParticleSwarm(object):
    def  __init__(self, num_particles, num_informants, objective_function, particle_length_continuous, num_discrete_variables = None, num_discrete_options = None, alpha = 0.9, beta = 0.6, gamma = 1.0, delta = 0.1, epsilon = 0.25, particles = None ):
        self.num_particles = num_particles # Number of particles in the swarm (swarm size)
        self.particle_length_continuous = particle_length_continuous # Length in the continuous search space
        self.particle_length_discrete = num_discrete_variables * num_discrete_options # Length in the discrete search space
        self.particle_length = particle_length_continuous + self.particle_length_discrete # Total particle length
        self.num_discrete_variables = num_discrete_variables # Number of discrete variables 
        self.num_discrete_options = num_discrete_options # Number of options per discrete variable
        self.objective_function = objective_function # Objective function to be minimized
        
        self.alpha = alpha # α (alpha): proportion of previous velocity to be retained 
        self.beta = beta # β (beta): proportion of personal best to be retained 
        self.gamma = gamma # γ (gamma): proportion of informants' best to be retained 
        self.delta = delta # δ (delta): proportion of global best to be retained 
        self.epsilon = epsilon # ε (epsilon): jump size or step size of a particle 
        
        # Visualization variables created by chatGPT
        self.Gbest_value_history = []
        self.mean_fitness_history = []
        self.particle_history = []
        self.velocity_history = []
        self.fitness_history = []
        self.Gbest_position_history = []
        
        
        if particles is not None:
            self.particle_array = particles # Initialise with existing particles 
        else:
            self.particle_array = np.random.uniform(size=(num_particles, self.particle_length)) # Initialise with random particle array

        # Here, the particle velocities are randomly initialized in a new array, where each velocity array index matches the particle array index
        # For example, particle_array[0] has a velocity of velocity_array[0]
        self.velocity_array = np.random.rand(num_particles, self.particle_length) # Stores particle velocities at each particle index

        # Stores personal best positions for each particle in the matching index
        self.personal_best_array = np.copy(self.particle_array)
        
        # Store informant best positions for each particle, where each informant best position matches the particle array index
        # For example,  particle_informant_best[0] stores the best informant position for particle_array[0]
        self.particle_informant_indices, self.particle_informant_best = self.create_particle_informant_best_nearest(num_informants)
        
        ## Expiriment 01 - informant selection scheme
        #self.particle_informant_indices, self.particle_informant_best = self.create_particle_informant_best_random(num_informants)
        
        # Stores fitness values for each particle in the matching index
        self.fitness_values_array =  self.objective_function(self.particle_array) 
        
        # Find and store global best position and value
        global_best_idx = np.argmin(self.fitness_values_array)
        self.Gbest = self.particle_array[global_best_idx]
        self.Gbest_value = self.fitness_values_array[global_best_idx]
        print(f"Initial Global Best Value: {self.Gbest_value}")
        
        

    
    # Generated by ChatGPT
    def create_particle_informant_best_random(self, num_informants):
        
        # --- Initialize the container for best informants per particle ---
        particle_informant_best = np.zeros_like(self.particle_array)
        particle_informant_indices = np.zeros((self.num_particles , num_informants))

        # --- Loop over each source particle ---
        for i in range(self.num_particles):

            # Select random unique indices excluding the current particle
            possible_indices = [j for j in range(self.num_particles) if j != i]
            informant_indices = np.random.choice(possible_indices,
                                                size=min(num_informants, len(possible_indices)),
                                                replace=False)
            # Extract the informant particles
            informants = self.particle_array[informant_indices]
            

            # Evaluate their fitness
            local_fitness_values_array = self.objective_function(informants)
            

            # Find the best informant among them
            best_idx_local = np.argmin(local_fitness_values_array)
            informant_best = informants[best_idx_local]
            # Store best informant position in array
    
            particle_informant_indices[i] = informant_indices
            particle_informant_best[i] = informant_best
            #print(f"Particle {i}: Informants indices {informant_indices}, Best informant index {best_idx_local}, Fitness {local_fitness_values_array[best_idx_local]}")

        # Optional: return for inspection
        return particle_informant_indices, particle_informant_best


    def create_particle_informant_best_nearest(self, num_informants):
        num_particles = self.num_particles
        particle_array = self.particle_array
        particle_informant_best = np.zeros_like(particle_array)
        particle_informant_indices = np.zeros((num_particles, num_informants), dtype=int)

        # --- Compute pairwise squared Euclidean distances ---
        # distance_matrix[i,j] = ||particle[i] - particle[j]||^2
        diff = particle_array[:, np.newaxis, :] - particle_array[np.newaxis, :, :]  # shape (N,N,D)
        distance_matrix = np.sum(diff ** 2, axis=2)  # shape (N, N)
        
        # --- Loop over each particle to select nearest informants ---
        for i in range(num_particles):
            distances = distance_matrix[i]
            distances[i] = np.inf  # exclude self

            # Indices of closest particles
            nearest_indices = np.argsort(distances)[:num_informants]
            particle_informant_indices[i] = nearest_indices

            # Extract those informant particles
            informants = particle_array[nearest_indices]

            # Evaluate fitness
            local_fitness_values_array = self.objective_function(informants)

            # Find best informant
            best_idx_local = np.argmin(local_fitness_values_array)
            particle_informant_best[i] = informants[best_idx_local]

        return particle_informant_indices, particle_informant_best

 
    def update_particle_informant_best(self):    
            # --- Loop over each source particle ---
        for i in range(self.num_particles):
            # Extract the informant particles
            informants = self.particle_array[self.particle_informant_indices[i].astype(int)]
            # Evaluate their fitness
            local_fitness_values_array = self.objective_function(informants)
            # Find the best informant among them
            best_idx_local = np.argmin(local_fitness_values_array)
            informant_best = informants[best_idx_local]
            # Store best informant position in array
            self.particle_informant_best[i] = informant_best

  
    def _update(self):

        cognitive_component_array =  np.random.uniform(low = 0, high = self.beta, size=(self.num_particles, self.particle_length))
        local_social_component_array = np.random.uniform(low = 0, high = self.gamma, size=(self.num_particles, self.particle_length))
        global_social_component_array = np.random.uniform(low = 0, high = self.delta, size=(self.num_particles, self.particle_length))

        
        self.velocity_array = (
            self.alpha * self.velocity_array 
            + cognitive_component_array * (self.personal_best_array - self.particle_array) 
            + local_social_component_array * (self.particle_informant_best - self.particle_array) 
            + global_social_component_array * (self.Gbest - self.particle_array)
        )
    
        self.particle_array = self.particle_array + self.epsilon * self.velocity_array
        
        ## Expiriment 02 - informant update scheme (Fixed or nearest)
        self.update_particle_informant_best()
                
        
        ## Generated by ChatGPT 
        # Normalize discrete variables in each particle to maintain probability distributions for each discrete variable
        num_c = self.particle_length_continuous       # continuous variable count
        num_d = self.num_discrete_variables         # discrete variable count
        num_o = self.num_discrete_options           # options per discrete variable
        # Extract discrete section and reshape
        discrete_section = self.particle_array[:, num_c:].reshape(self.num_particles, num_d, num_o)
        # Normalize each probability distribution along the last axis
        discrete_sum = discrete_section.sum(axis=2, keepdims=True)
        discrete_sum[discrete_sum == 0] = 1.0   # avoid division by zero
        discrete_section /= discrete_sum
        # Write it back inline
        self.particle_array[:, num_c:] = discrete_section.reshape(self.num_particles, -1)

        # Calculate fitness values and update personal best array
        self.fitness_values_array = self.objective_function(self.particle_array)
        improved_particles_idx = self.fitness_values_array < self.objective_function(self.personal_best_array)
        self.personal_best_array[improved_particles_idx] = self.particle_array[improved_particles_idx]
        
        ## Version to set gbest based on the current particle fitness values
        # Update global best position and value
        global_best_idx = np.argmin(self.fitness_values_array)
        if self.fitness_values_array[global_best_idx] < self.Gbest_value:
            self.Gbest = self.particle_array[global_best_idx]
            self.Gbest_value = self.fitness_values_array[global_best_idx]
        
        
        # Version to set gbest based on only the partilces personal best fitness values
        # pb_fitness_values_array = self.objective_function(self.personal_best_array)
        # global_best_idx = np.argmin(pb_fitness_values_array)
        # if pb_fitness_values_array[global_best_idx] < self.Gbest_value:
        #     self.Gbest = self.particle_array[global_best_idx]
        #     self.Gbest_value = pb_fitness_values_array[global_best_idx]
            

        # --- Record for visualization --- genereated by chatgpt
        mean_fitness = np.mean(self.fitness_values_array)
        mean_fitness = min(mean_fitness, 5000)  # <-- clip to a max of 5000
        self.mean_fitness_history.append(mean_fitness)
        self.Gbest_value_history.append(self.Gbest_value)
        self.particle_history.append(self.particle_array.copy())
        self.velocity_history.append(self.velocity_array.copy())
        self.fitness_history.append(self.fitness_values_array.copy())
        self.Gbest_position_history.append(self.Gbest.copy())
        
    # -------------------------------------------------------------
    # VISUALIZATION UTILITIES
    # -------------------------------------------------------------

    def plot_convergence(self):
        """Plot global best and mean fitness convergence."""
        plt.figure(figsize=(8, 5))
        plt.plot(self.Gbest_value_history, label="Global Best")
        plt.plot(self.mean_fitness_history, label="Mean Fitness", linestyle="--")
        plt.xlabel("Iteration")
        plt.ylabel("Fitness (MSE)")
        plt.title("PSO Convergence Curve")
        plt.legend()
        plt.grid(True)
        plt.show()

    def plot_swarm_diversity(self):
        """Plot average swarm diversity across iterations."""
        diversities = []
        for particles in self.particle_history:
            mean_pos = np.mean(particles, axis=0)
            diversity = np.mean(np.linalg.norm(particles - mean_pos, axis=1))
            diversities.append(diversity)

        plt.figure(figsize=(8, 5))
        plt.plot(diversities, color="orange")
        plt.xlabel("Iteration")
        plt.ylabel("Average Diversity")
        plt.title("Swarm Diversity Over Time")
        plt.grid(True)
        plt.show()

    def plot_velocity_magnitude(self):
        """Plot average particle velocity magnitude."""
        avg_vel = [np.mean(np.linalg.norm(v, axis=1)) for v in self.velocity_history]
        plt.figure(figsize=(8, 5))
        plt.plot(avg_vel, color="purple")
        plt.xlabel("Iteration")
        plt.ylabel("Average Velocity Magnitude")
        plt.title("Swarm Velocity Magnitude Over Time")
        plt.grid(True)
        plt.show()

    def animate_pso_pca(self):
        """Animate particle swarm movement projected to 2D via PCA."""
        all_particles = np.vstack(self.particle_history)
        pca = PCA(n_components=2)
        pca.fit(all_particles)

        # Compute bounds dynamically
        proj_all = pca.transform(all_particles)
        x_min, x_max = proj_all[:, 0].min(), proj_all[:, 0].max()
        y_min, y_max = proj_all[:, 1].min(), proj_all[:, 1].max()

        fig, ax = plt.subplots(figsize=(6, 6))
        scat = ax.scatter([], [], c="blue", alpha=0.6)
        gb_point, = ax.plot([], [], "r*", markersize=12, label="Global Best")

        def init():
            ax.set_xlim(x_min, x_max)
            ax.set_ylim(y_min, y_max)
            ax.set_title("PSO Swarm Dynamics (PCA Projection)")
            return scat, gb_point

        def update(frame):
            particles = pca.transform(self.particle_history[frame])
            gbest = pca.transform(self.Gbest_position_history[frame].reshape(1, -1))
            scat.set_offsets(particles)
            gb_point.set_data(gbest[0, 0], gbest[0, 1])
            ax.set_title(f"Iteration {frame + 1}")
            return scat, gb_point

        ani = FuncAnimation(fig, update, frames=len(self.particle_history),
                            init_func=init, blit=False, repeat=False)
        plt.legend()
        plt.show()

    def visualize_particle_structure(self, iteration=-1):
        """Inspect continuous (weights/biases) vs discrete (activation probabilities)."""
        particles = self.particle_history[iteration]
        continuous = particles[:, :self.particle_length_continuous]
        discrete = particles[:, self.particle_length_continuous:]

        # --- Parallel coordinates for continuous variables ---
        df_cont = sns.load_dataset("iris").iloc[:self.num_particles, :continuous.shape[1] - 1]  # dummy fallback if seaborn fails
        try:
            df_cont = pd.DataFrame(continuous)
            df_cont["fitness"] = self.fitness_history[iteration]
            plt.figure(figsize=(10, 4))
            sns.parallel_coordinates(df_cont, "fitness", colormap="viridis", alpha=0.5)
            plt.title(f"Parallel Coordinates (Continuous Variables) – Iter {iteration + 1}")
            plt.xlabel("Parameter Index")
            plt.ylabel("Value")
            plt.show()
        except Exception:
            print("Parallel coordinates skipped (too many dimensions or missing pandas).")

        # --- Heatmap for discrete variables ---
        discrete = discrete.reshape(self.num_particles, self.num_discrete_variables, self.num_discrete_options)
        avg_discrete = np.mean(discrete, axis=0).T  # Average across particles

        plt.figure(figsize=(8, 4))
        sns.heatmap(avg_discrete, annot=False, cmap="coolwarm")
        plt.title(f"Average Discrete Variable Probabilities (Iteration {iteration + 1})")
        plt.xlabel("Discrete Variable Index")
        plt.ylabel("Activation Option Index")
        plt.show()
        
            
    
    


 
