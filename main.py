import functools
import numpy as np
import pandas as pd

import feed_forward
import pso

path = "concrete_data.csv"
dataset = pd.read_csv(path)

split_idx = int(dataset.shape[0] * 0.7) # iloc takes int and not floats
training_dataset = dataset.iloc[:split_idx, :] 
testing_dataset = dataset.iloc[split_idx:, :]

training_input = training_dataset.iloc[:, :8].values


    
def initialize_particles(ann):

    particles = []
    
    # Extract ann structure
    num_layers = ann.num_layers
    num_activations_variables = num_layers - 2 # Number of activation function variables to optimize
    num_activation_functions = ann.num_activation_functions # Number of available activation functions
    
    # --- Create random weights & biases for each layer ---
    for _ in range(num_particles):
        weights = ann.weights
        biases = ann.biases
        activations = np.random.randint(0, num_activation_functions, size=(num_activations_variables,))
        # Generated by ChatGPT
        # --- Flatten and concatenate all into one long vector ---
        flat_vector = np.concatenate(
            [W.flatten() for W in weights] +
            [b.flatten() for b in biases] +
            [activations.flatten()]
        )
        
        
        particles.append(flat_vector)

    
        
        
    return np.array(particles)

# Generated by ChatGPT
def reconstruct_params(flat_vector, ann):
    weights, biases, activations = [], [], []
    idx = 0

    layers = ann.layers
    num_layers = ann.num_layers
    num_activations_variables = num_layers - 2


    # Extract all weights
    for i in range(num_layers - 1):
        w_size = layers[i] * layers[i+1]
        W = flat_vector[idx:idx+w_size].reshape(layers[i], layers[i+1])
        idx += w_size
        weights.append(W)

    # Extract all biases
    for i in range(num_layers - 1):
        b_size = layers[i+1]
        b = flat_vector[idx:idx+b_size].reshape(1, layers[i+1])
        idx += b_size
        biases.append(b)

    # Extract all activation functions
    a_size = num_activations_variables
    activations = flat_vector[idx:idx+a_size].astype(int).tolist()

    return [weights, biases, activations]

def objective_function(ann, layers, particles):
    mse_array = []
    for particle in particles:
        params = reconstruct_params(particle, layers)
        predictions = ann._forward(training_input, params=params)

        # # ABDUL
        # # Update ann class to return loss value based on selected loss function
        # # Example code
        # loss = ann._loss(predictions, training_dataset.iloc[:, 8:].values)
        # loss_array.append(loss)
        
        mse = np.mean((predictions - training_dataset.iloc[:, 8:].values) ** 2)
        mse_array.append(mse)
        
        #print(f"MSE: {mse}")
    return np.array(mse_array)

def optimize_pso(num_iterations, pso):
    for iteration in range(num_iterations):
        pso._update()
        print(f"Iteration {iteration+1}/{num_iterations}, Global Best MSE: {pso.Gbest_value}")
    
    

layers = [8, 16, 16, 1]
activations = ['relu','relu']
ann = feed_forward.MultiLayerANN(layers, activations)

num_particles = 50
num_iterations = 100
num_informants = 10

particles = initialize_particles(ann)

obj_func = functools.partial(objective_function, ann, layers, activations)
pso = pso.ParticleSwarm(num_particles,num_informants, particles.shape[1], obj_func, particles=particles)
optimize_pso(num_iterations, pso)

# reconstructed_params = reconstruct_params(particles[0], ann)
# predictions = ann._forward(training_input, reconstructed_params)
#print(predictions)
    