import functools
import numpy as np
import pandas as pd

import feed_forward
import pso

path = "concrete_data.csv"
dataset = pd.read_csv(path)

split_idx = int(dataset.shape[0] * 0.7) # iloc takes int and not floats
training_dataset = dataset.iloc[:split_idx, :] 
testing_dataset = dataset.iloc[split_idx:, :]

training_input = training_dataset.iloc[:, :8].values


    
def initialize_particles(layers,num_particles):

    particles = []
    # --- Create random weights & biases for each layer ---
    for _ in range(num_particles):
        weights = []
        biases = []
        for i in range(len(layers) - 1):
            weights.append(np.random.randn(layers[i], layers[i+1]) * 0.1)
            biases.append(np.zeros((1, layers[i+1])))
    
        # Generated by ChatGPT
        # --- Flatten and concatenate all into one long vector ---
        flat_vector = np.concatenate(
            [W.flatten() for W in weights] +
            [b.flatten() for b in biases]
        )
        
        particles.append(flat_vector)
        
    return np.array(particles)

# Generated by ChatGPT
def reconstruct_params(flat_vector, layers):
    weights, biases = [], []
    idx = 0

    # First extract all weights
    for i in range(len(layers) - 1):
        w_size = layers[i] * layers[i+1]
        W = flat_vector[idx:idx+w_size].reshape(layers[i], layers[i+1])
        idx += w_size
        weights.append(W)

    # Then extract all biases
    for i in range(len(layers) - 1):
        b_size = layers[i+1]
        b = flat_vector[idx:idx+b_size].reshape(1, layers[i+1])
        idx += b_size
        biases.append(b)

    return [weights, biases]

def objective_function(layers, activations, particles):
    mse_array = []
    for particle in particles:
        params = reconstruct_params(particle, layers)
        ann = feed_forward.MultiLayerANN(layers, activations, params=params)
        predictions = ann._forward(training_input)
        mse = np.mean((predictions - training_dataset.iloc[:, 8:].values) ** 2)
        mse_array.append(mse)
        #print(f"MSE: {mse}")
    return np.array(mse_array)

def optimize_pso(num_iterations, pso):
    for iteration in range(num_iterations):
        pso._update()
        print(f"Iteration {iteration+1}/{num_iterations}, Global Best MSE: {pso.Gbest_value}")
    
    

layers = [8, 16, 16, 1]
activations = ['relu', 'relu']

num_particles = 50
num_iterations = 100
num_informants = 10

particles = initialize_particles(layers, num_particles)
obj_func = functools.partial(objective_function, layers, activations)
pso = pso.ParticleSwarm(num_particles,num_informants, particles.shape[1], obj_func, particles=particles)
optimize_pso(num_iterations, pso)

#reconstructed_params = reconstruct_params(particles[0], layers)
# ann = feed_forward.MultiLayerANN(layers, activations, params=reconstructed_params)
# predictions = ann._forward(training_input)
# print(predictions)
    