import numpy as np
import loss_functions

def initialize_particles(ann, num_particles):
    """
    Creates an initial population of particles for PSO.
    Each particle is a flat 1D vector encoding all ANN parameters 
    (weights, biases, and activation function indices).
    """
    particles = []

    num_layers = ann.num_layers
    # Number of learnable activation functions (one for each hidden layer)
    num_activation_variables = num_layers - 2 
    num_activation_functions = ann.num_activation_functions
    

    for _ in range(num_particles):
        weights = []
        biases = []
        
        # Initialize weights and biases for all layers
        for i in range(num_layers - 1):
            # Small random weights
            weights.append(np.random.randn(ann.layers[i], ann.layers[i+1]) * 0.1) 
            # Zero biases
            biases.append(np.zeros((1, ann.layers[i+1])))
       
        
        discrete_variable_probabilities = []
        for _ in range(num_activation_variables):
            probability_array = np.random.rand(1, num_activation_functions)
            probability_array /= probability_array.sum()  # Normalize to sum to 1
            discrete_variable_probabilities.append(probability_array.flatten())
            #print(f"Discrete Variable Probability Array: {probability_array}")
            
        discrete_variable_probabilities = np.array(discrete_variable_probabilities)
        
        
        # Generated by ChatGPT
        # --- Flatten and concatenate all into one long vector ---
        flat_vector = np.concatenate(
            [W.flatten() for W in weights] +
            [b.flatten() for b in biases] +
            [discrete_variable_probabilities.flatten()]
        )  
        particles.append(flat_vector)
    
    particles = np.array(particles)
    particle_length = particles.shape[1]
    discrete_variable_length = num_activation_variables * num_activation_functions
    discrete_variable_params = [num_activation_variables, num_activation_functions]
    print(f"Particle Length: {particle_length} (Continuous: {particle_length - discrete_variable_length}, Discrete: {discrete_variable_length})")
    
    return particles, particle_length, discrete_variable_params

def reconstruct_params(flat_vector, ann):
    """
    Converts a flat 1D particle vector back into the ANN's structured
    parameters (weights, biases, activation list).
    """
    weights, biases, activations = [], [], []
    idx = 0

    layers = ann.layers
    num_layers = ann.num_layers
    activations_functions = ann.activations_functions
    num_activation_functions = ann.num_activation_functions
    num_activation_variables = num_layers - 2
    

    # Extract all weights
    for i in range(num_layers - 1):
        w_size = layers[i] * layers[i+1]
        W = np.array(flat_vector[idx:idx+w_size], dtype=float).reshape(layers[i], layers[i+1])
        idx += w_size
        weights.append(W)

    # Extract all biases
    for i in range(num_layers - 1):
        b_size = layers[i+1]
        b = np.array(flat_vector[idx:idx+b_size], dtype=float).reshape(1, layers[i+1])
        idx += b_size
        biases.append(b)

    # Extract all activation functions
    a_size = num_activation_variables * num_activation_functions
    discrete_variable_probabilities = flat_vector[idx:idx+a_size]
    
    #Select the activation with the highest probability for each variable
    activations = np.array([
        activations_functions[np.argmax(prob_array)]
        for prob_array in discrete_variable_probabilities.reshape(num_activation_variables, num_activation_functions)
    ])
    

    return [weights, biases, activations]

# 2. ADD NEW PARAM
def create_objective_function(ann, X_train, y_train, loss_function_name='mse'): 
    """
    A "factory" that returns the objective function the PSO will minimize.
    This function "closes over" the ann, X_train, and y_train variables
    so they are available to the inner function.
    """
    
    # 3. Get the actual loss function from the registry
    loss_func = loss_functions.get_loss_function(loss_function_name)
    
    def objective_function(particles):
        """
        Calculates the fitness (loss) for a batch of particles.
        This is the function that PSO will call repeatedly.
        """
        loss_array = []
        for particle in particles:
            # 1. Reconstruct the ANN parameters from the particle's 1D vector
            params = reconstruct_params(particle, ann)
            
            # 2. Get model predictions using these parameters
            predictions = ann.evaluate_with_params(X_train, params=params)
            
            # 3. Calculate error (fitness) using the chosen loss function
            # 4. USE THE CHOSEN FUNC
            loss = loss_func(y_train, predictions) 
            loss_array.append(loss)
            
        return np.array(loss_array)

    # Return the *function itself*, not the result of calling it.
    return objective_function