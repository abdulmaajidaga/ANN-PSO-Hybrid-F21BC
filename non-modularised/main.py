import functools
import numpy as np
import pandas as pd

import feed_forward
import pso

path = "concrete_data.csv"
dataset = pd.read_csv(path)

split_idx = int(dataset.shape[0] * 0.7) # iloc takes int and not floats
training_dataset = dataset.iloc[:split_idx, :] 
testing_dataset = dataset.iloc[split_idx:, :]

training_input = training_dataset.iloc[:, :8].values


    
def initialize_particles(ann):

    particles = []
    
    # Extract ann structure
    num_layers = ann.num_layers
    activation_options = ann.activation_options # Number of available activation options
    num_activation_options = len(activation_options)
    weights = ann.weights
    biases = ann.biases
    
    
    # Calculate Mixed PSO parameters
    continuous_variables_length = sum([w.size for w in weights]) + sum([b.size for b in biases])
    discrete_variables_length = num_layers - 2 # Number of activation function variables to optimize
    
    

    # --- Create random weights & biases for each layer ---
    for _ in range(num_particles):

        discrete_variable_probabilities = []
        for _ in range(discrete_variables_length):
            probability_array = np.random.rand(1, num_activation_options)
            probability_array /= probability_array.sum()  # Normalize to sum to 1
            discrete_variable_probabilities.append(probability_array.flatten())
            #print(f"Discrete Variable Probability Array: {probability_array}")
            
        discrete_variable_probabilities = np.array(discrete_variable_probabilities)
        
        
        
        # Select the activation with the highest probability for each variable
        # activations = np.array([
        #     activation_options[np.argmax(prob_array)]
        #     for prob_array in discrete_variable_probabilities
        # ])
        
        # Generated by ChatGPT
        # --- Flatten and concatenate all into one long vector ---
        flat_vector = np.concatenate(
            [W.flatten() for W in weights] +
            [b.flatten() for b in biases] +
            [discrete_variable_probabilities.flatten()]
        )  
        particles.append(flat_vector)
        
    

    print(f"Particle Length: {continuous_variables_length + discrete_variables_length*num_activation_options} (Continuous: {continuous_variables_length}, Discrete: {discrete_variables_length*num_activation_options })")
    return np.array(particles), continuous_variables_length, discrete_variables_length, num_activation_options

# Generated by ChatGPT
def reconstruct_params(flat_vector, ann):
    weights, biases, activations = [], [], []
    idx = 0

    layers = ann.layers
    num_layers = ann.num_layers
    activation_options = ann.activation_options
    discrete_variables_length = num_layers - 2


    # Extract all weights
    for i in range(num_layers - 1):
        w_size = layers[i] * layers[i+1]
        W = np.array(flat_vector[idx:idx+w_size], dtype=float).reshape(layers[i], layers[i+1])
        idx += w_size
        weights.append(W)

    # Extract all biases
    for i in range(num_layers - 1):
        b_size = layers[i+1]
        b = np.array(flat_vector[idx:idx+b_size], dtype=float).reshape(1, layers[i+1])
        idx += b_size
        biases.append(b)

    # Extract all activation functions
    a_size = discrete_variables_length * len(activation_options)
    discrete_variable_probabilities = flat_vector[idx:idx+a_size]

    #Select the activation with the highest probability for each variable
    activations = np.array([
        activation_options[np.argmax(prob_array)]
        for prob_array in discrete_variable_probabilities.reshape(discrete_variables_length, len(activation_options))
    ])
    

    return [weights, biases, activations]

def objective_function(ann, layers, particles):
    mse_array = []
    for particle in particles:
        params = reconstruct_params(particle, ann)
        predictions = ann._forward(training_input, params=params)

        # # ABDUL
        # # Update ann class to return loss value based on selected loss function
        # # Example code
        # loss = ann._loss(predictions, training_dataset.iloc[:, 8:].values)
        # loss_array.append(loss)
        
       
        mse = np.mean((predictions - training_dataset.iloc[:, 8:].values) ** 2)
        mse_array.append(mse)
        
        #print(f"MSE: {mse}")
    return np.array(mse_array)

def optimize_pso(num_iterations, pso):
    for iteration in range(num_iterations):
        pso._update()
        print(f"Iteration {iteration+1}/{num_iterations}, Global Best MSE: {pso.Gbest_value}, Mean MSE: {np.mean(pso.fitness_values_array)}")
    
    

layers = [8, 16, 16, 1]
ann = feed_forward.MultiLayerANN(layers)

num_particles = 50
num_iterations = 200
num_informants = 10

particles , continuous_num, discrete_num, discrete_options = initialize_particles(ann)


obj_func = functools.partial(objective_function, ann, layers)
pso = pso.ParticleSwarm(num_particles, num_informants,obj_func, continuous_num,num_discrete_variables=discrete_num, num_discrete_options= discrete_options, particles=particles)
optimize_pso(num_iterations, pso)


# Visualizations
#pso.plot_convergence()
#pso.plot_swarm_diversity()
#pso.plot_velocity_magnitude()
pso.animate_pso_pca()
#pso.visualize_particle_structure(iteration=-1)

# reconstructed_params = reconstruct_params(particles[-1], ann)
# print(reconstructed_params[-1])
# predictions = ann._forward(training_input, reconstructed_params)
# print(predictions)
    