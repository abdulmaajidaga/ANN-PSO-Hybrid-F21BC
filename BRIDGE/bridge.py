import numpy as np
import ANN.loss_functions as loss_functions


class Bridge(object):
    def  __init__(self, ann, discrete = False):
        self.discrete = discrete
        self.ann = ann
        
    def initialize_particles(self, num_particles):
        """
        Creates an initial population of particles for PSO.
        Each particle is a flat 1D vector encoding all ANN parameters 
        (weights, biases, and activation function indices).
        """
        particles = []

        num_layers = self.ann.num_layers
        # Number of learnable activation functions (one for each hidden layer)
        num_activation_variables = num_layers - 2 
        num_activation_functions = self.ann.num_activation_functions
        

        for _ in range(num_particles):
            weights = []
            biases = []
            
            # Initialize weights and biases for all layers
            for i in range(num_layers - 1):
                # Small random weights
                weights.append(np.random.randn(self.ann.layers[i], self.ann.layers[i+1]) * 0.1) 
                # Zero biases
                biases.append(np.zeros((1, self.ann.layers[i+1])))
        
            # Generated by ChatGPT
            # --- Flatten and concatenate all into one long vector ---
            flat_vector = np.concatenate(
                [W.flatten() for W in weights] +
                [b.flatten() for b in biases]
            ) 
            
            
            if self.discrete:
                discrete_variable_probabilities = []
                for _ in range(num_activation_variables):
                    probability_array = np.random.rand(1, num_activation_functions)
                    probability_array /= probability_array.sum()  # Normalize to sum to 1
                    discrete_variable_probabilities.append(probability_array.flatten())
                    
                discrete_variable_probabilities = np.array(discrete_variable_probabilities)
                flat_vector = np.concatenate([flat_vector, discrete_variable_probabilities.flatten()])
                
            
            particles.append(flat_vector)
        
        particles = np.array(particles)
        particle_length = particles.shape[1]
        discrete_variable_length = num_activation_variables * num_activation_functions
        discrete_variable_params = [num_activation_variables, num_activation_functions]
        print(f"Particle Length: {particle_length} (Continuous: {particle_length - discrete_variable_length}, Discrete: {discrete_variable_length})")
        
        return particles, particle_length, discrete_variable_params
    
    def reconstruct_params(self, flat_vector):
        """
        Converts a flat 1D particle vector back into the ANN's structured
        parameters (weights, biases, activation list).
        """
        weights, biases, activations = [], [], []
        idx = 0

        layers = self.ann.layers
        num_layers = self.ann.num_layers
        activations_functions = self.ann.activations_functions
        num_activation_functions = self.ann.num_activation_functions
        num_activation_variables = num_layers - 2
        

        # Extract all weights
        for i in range(num_layers - 1):
            w_size = layers[i] * layers[i+1]
            W = np.array(flat_vector[idx:idx+w_size], dtype=float).reshape(layers[i], layers[i+1])
            idx += w_size
            weights.append(W)

        # Extract all biases
        for i in range(num_layers - 1):
            b_size = layers[i+1]
            b = np.array(flat_vector[idx:idx+b_size], dtype=float).reshape(1, layers[i+1])
            idx += b_size
            biases.append(b)

        if self.discrete:
            # Extract all activation functions
            a_size = num_activation_variables * num_activation_functions
            discrete_variable_probabilities = flat_vector[idx:idx+a_size]
            
            #Select the activation with the highest probability for each variable
            activations = np.array([
                activations_functions[np.argmax(prob_array)]
                for prob_array in discrete_variable_probabilities.reshape(num_activation_variables, num_activation_functions)
            ])
            
            return [weights, biases, activations]
        return [weights, biases]

    def create_objective_function(self, X_train, y_train,  loss_function_name='mse'): 
        """
        A "factory" that returns the objective function the PSO will minimize.
        This function "closes over" the ann, X_train, and y_train variables
        so they are available to the inner function.
        """
        
        # Get the actual loss function from the registry
        loss_func = loss_functions.get_loss_function(loss_function_name)
        
        def objective_function(particles):
            """
            Calculates the fitness (loss) for a batch of particles.
            This is the function that PSO will call repeatedly.
            """
            loss_array = []
            for particle in particles:
                # 1. Reconstruct the ANN parameters from the particle's 1D vector
                params = self.reconstruct_params(particle)
                # 2. Get model predictions using these parameters
                predictions = self.ann.evaluate_with_params(X_train, params=params)
                # 3. Calculate error (fitness) using the chosen loss function
                loss = loss_func(y_train, predictions) 
                loss_array.append(loss)
                
            return np.array(loss_array)

        # Return the *function itself*, not the result of calling it.
        return objective_function